{
  "Workers": [
    // -----------------------------
    // Audio extraction
    {
      "Id": "audio",
      "Enabled": true
    },
    {
      "Id": "audio-clean-waveform",
      "Enabled": true,
    },
    {
      "Id": "asig",
      "Enabled": true     
    },
    // -----------------------------
    // Creating initial manual-input source 
    {
      "Id": "full-ai",
      "Enabled": true,
      "MaxChunkDuration": "00:05:00",
      "ExportMetadataSrt": false,
      "TranscriptionToIgnorePatterns" : [ 
        "あ+[ぁ]*。",
        "ん[っー]*。",
        "うーん。"
        ]
    },
    {
      "Id": "full-ai-refined",
      // [OPTION] Enable to get a second pass on that should improve the timings (i.e. start and end) of the subtitles, and add 'cadence marker' in the text ("How about..8..we go in my bedroom?").
      // Note: It's less useful for this workflow since the start and end should be adjusted manually and another transcription will be done that override the cadence markers.
      "Enabled": false,
      "ExportMetadataSrt": false
    },
    {
      "Id": "full",
      "Enabled": true,
      // [OPTION] Change to "full-ai-refined" if you enabled it above
      "SourceId": "full-ai"
    },
    {
      "Id": "generated-manual-input-srt",
      "Enabled": true
    },
    {
      "Id": "manual-input",
      "Enabled": true
    },
    // -----------------------------
    // ManualHQWorkflow: Finalizing timings and voice-texts sources
    {
      "Id": "timings",
      "Enabled": true
    },
    {
      "Id": "singlevad-ai",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        // [OPTION] Increase the batch size to reduce the number of request but it also mean that there will be less 'thinking' time allocated per node.
        "BatchSize": 100,
        "BatchSplitWindows": 5,
        "NbContextItems": 10,
        "NbItemsMinimumReceivedToContinue": 40
      }
    },
    {
      "Id": "singlevad-ai-refined",
      // Note: this is not the same refininement as full. In this case, the AI will validate that the transcription by looking at the initial transcription and the full-ai transcription. It also receive and analyze screenshot to create a better TranslationAnalysis.
      // [OPTION] Disable if you want to reduce the API cost.
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        "BinaryDataExtractors": [
          {
            "OutputFieldName": "Screenshot",
            // [OPTION] Set AddContextNodes to 'false' to reduce the size of requests, or increase the Gap settings to have less nodes added. 
            // Short explanation, if there is a large gap between two subtitles, the following context nodes will be added: 
            //    <previous>.EndTime + <ContextShortGap>, ...images every <ContextLongGap>..., <current>.Start Time - <ContextShortGap>.
            "Enabled": true,
            "AddContextNodes": true,
            "ContextShortGap": "00:00:05",
            "ContextLongGap": "00:00:30"
          }
        ],
        // [OPTION] Increase the batch size to reduce the number of request but it also mean that there will be less 'thinking' time allocated per node.
        "BatchSize": 16,
        "BatchSplitWindows": 2,
        "NbContextItems": 100,
        "NbItemsMinimumReceivedToContinue": 8
      }
    },
    {
      "Id": "voice-texts",
      "Enabled": true,
      // [OPTION] Change to "singlevad-ai" if you disabled 'refined' above
      "SourceId": "singlevad-ai-refined"
    },    
    // -----------------------------
    // Use timings, manual-input and voice-texts to mergedvad, speaker, on-screen-texts, visual-analysis
    {
      "Id": "mergedvad",
      "Enabled": false
    },
    {
      "Id": "speakers",
      "Enabled": true
      // [OPTION] Set to false if you don't want to take the time to manually identify the speakers.
    },
    {
      "Id": "on-screen-texts",
      "Enabled": true
    },
    {
      "Id": "visual-analysis",
      "Enabled": false
      // [OPTION] =>  Set to true to use full visual analysis (even though singlevad-ai-refined already does a visual analysis).
    },
    // -----------------------------
    // Advanced step: Allowing the user to review and override AI generated metadatas
    {
      "Id": "metadatas-srt",
      "Enabled": true
    },
    {
      "Id": "metadatas-review",
      "Enabled": false
      // [OPTION] Set to true if you want to review the metadatas generated by the AI.
    },
        
    // -----------------------------
    // Use timings, manual-input, voice-texts and AI generated metadatas to AI Translation & Arbitration
    {
      "Id": "translated-texts_maverick",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        "BatchSize": 75,
        "BatchSplitWindows": 10,
        "NbContextItems": 100,
        "NbItemsMinimumReceivedToContinue": 40,
      }
    },
    {
      "Id": "finalized_maverick",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "MaxLineLength": 50,
      "MaxMergeGapSameSpeaker": "00:00:01",
      "MaxMergeGapDialogue": "00:00:01.2000000",
      "MaxMergeDuration": "00:00:06",
      "RemoveMissing": false,
      "AddReviewIfTooLong": true,
      "SplitPatterns": [
        { "CutWhere": "After", "Pattern": "\\.|[?!。！？]+" },
        { "CutWhere": "After", "Pattern": "[,.?!。、！:？]+" },
        { "CutWhere": "Before", "Pattern": "\\b(and|but|or)\\b" },
        { "CutWhere": "Before", "Pattern": "\\s" }
      ]
    },
    {
      "TranscriptionId": "final-ai-texts",
      "Enabled": true,
      // [OPTION] Set to "arbitrer-final-choice" if you want to go with multiples translation and an arbiter.
      "SourceId": "finalized_maverick"
    },
    {
      "Id": "final-ai-srt",
      "Enabled": true
    },
    {
      "Id": "final-ai-debug-srt",
      "Enabled": true
    },
    {
      "Id": "costs",
      "Enabled": true
    },
    {
      "TranscriptionId": "final-user-texts",
      // [OPTION] Set to true if you want to go import ".final-user.srt" file (final version after manual edit).
      "Enabled": false
    },
    {
      "OutputId": "learning-srt",
      // [OPTION] Set to true if you want to go import ".final-user.srt" file (final version after manual edit) and output a subtitle that, maybe, could be used to help an AI learn your preference.
      "Enabled": false
    },
  ]
}
