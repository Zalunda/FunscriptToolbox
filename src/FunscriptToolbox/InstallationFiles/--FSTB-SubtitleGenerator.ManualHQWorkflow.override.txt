{
  "Workers": [
    // -----------------------------
    // Audio extraction
    {
      "Id": "audio",
      "Enabled": true
    },
    {
      "Id": "audio-clean-waveform",
      "Enabled": true,
    },
    {
      "Id": "asig",
      "Enabled": true     
    },
    // -----------------------------
    // Creating initial manual-input source 
    {
      "Id": "full-ai",
      "Enabled": true,
      "MaxChunkDuration": "00:05:00",
      "ExportMetadataSrt": false,
      "TranscriptionToIgnorePatterns" : [ 
        "あ+[ぁ]*。",
        "ん[っー]*。",
        "うーん。"
        ]
    },
    {
      "Id": "full",
      "Enabled": true,
      // [OPTION] Change to "full-ai-refined" if you enabled it above
      "SourceId": "full-ai"
    },
    {
      "Id": "generated-manual-input-srt",
      "Enabled": true
    },
    {
      "Id": "manual-input",
      "Enabled": true
    },
    // -----------------------------
    // ManualHQWorkflow: Finalizing timings and voice-texts sources
    {
      "Id": "timings",
      "Enabled": true
    },
    {
      "Id": "on-screen-texts",
      "Enabled": true
    },
    {
      "Id": "singlevad-ai",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        // [OPTION] Increase the batch size to reduce the number of request but it also mean that there will be less 'thinking' time allocated per node.
        "BatchSize": 100,
        "BatchSplitWindows": 5,
        "NbContextItems": 10,
        "NbItemsMinimumReceivedToContinue": 40
      }
    },
    {
      "Id": "speakers",
      "Enabled": true
      // [OPTION] Set to false if you don't want to take the time to manually identify the speakers.
    },
    {
      "Id": "singlevad-ai-refined",
      // This AI will receive the audio and validate that the transcription by looking at the initial transcription and the full-ai transcription. It also receive and analyze screenshot to create a better TranslationAnalysis.
      // [OPTION] Disable if you want to reduce the API cost.
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        "BinaryDataExtractors": [
          {
            "OutputFieldName": "Screenshot",
            // [OPTION] Set AddContextNodes to 'false' to reduce the size of requests, or increase the Gap settings to have less nodes added. 
            // Short explanation, if there is a large gap between two subtitles, the following context nodes will be added: 
            //    <previous>.EndTime + <ContextShortGap>, ...images every <ContextLongGap>..., <current>.Start Time - <ContextShortGap>.
            "Enabled": true,
            "AddContextNodes": true,
            "ContextShortGap": "00:00:05",
            "ContextLongGap": "00:00:30"
          }
        ],
        // [OPTION] Increase the batch size to reduce the number of request but it also mean that there will be less 'thinking' time allocated per node.
        "BatchSize": 16,
        "BatchSplitWindows": 2,
        "NbContextItems": 100,
        "NbItemsMinimumReceivedToContinue": 8
      }
    },
    {
      "Id": "voice-texts",
      "Enabled": true,
      // [OPTION] Change to "singlevad-ai" if you disabled 'refined' above
      "SourceId": "singlevad-ai-refined"
    },    
    // -----------------------------
    // Use timings, manual-input and voice-texts to mergedvad, speaker, on-screen-texts, visual-analysis
    {
      "Id": "mergedvad",
      "Enabled": false
    },
    // -----------------------------
    // Advanced step: Allowing the user to review and override AI generated metadatas
    {
      "Id": "metadatas-srt",
      "Enabled": true
    },
    {
      "Id": "metadatas-review",
      "Enabled": false
      // [OPTION] Set to true if you want to review the metadatas generated by the AI.
    },
        
    // -----------------------------
    // Use timings, manual-input, voice-texts and AI generated metadatas to AI Translation & Arbitration
    {
      "Id": "translated-texts_maverick",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        "BatchSize": 75,
        "BatchSplitWindows": 5,
        "NbContextItems": 400,
        "NbItemsMinimumReceivedToContinue": 50
      }
    },
    {
      "Id": "finalized_maverick",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "MaxLineLength": 50,
      "MaxMergeGapSameSpeaker": "00:00:01",
      "MaxMergeGapDialogue": "00:00:01.2000000",
      "MaxMergeDuration": "00:00:06",
      "RemoveMissing": false,
      "AddReviewIfTooLong": true,
      "SplitPatterns": [
        // 1. Strong Punctuation (End of sentence)
        { "CutWhere": "After", "Pattern": "(?<!\\.)\\.(?![\\d\\.])|[?!…]+" },
        // 2. Weak Punctuation (Commas, colons)
        { "CutWhere": "After", "Pattern": "\\.{3}|[,:：]+" },
        // 3. Logical/Casual Connectors (High quality splits)
        //    distinct from 'and/but/or', these usually don't require a preceding comma
        { "CutWhere": "Before", "Pattern": "\\b(because|if|since|while|although|until|unless|so|that)\\b" },
        // 4. "Wh-" Clause Connectors (Medium quality splits)
        //    Good for sentences like "I don't know [what to do]"
        { "CutWhere": "Before", "Pattern": "\\b(what|when|where|how|why)\\b" },
        // 5. Prepositions (Fallback for long sentences)
        //    Only use this if you really need to cut a long line
        { "CutWhere": "Before", "Pattern": "\\b(with|about|for|from|into)\\b" },
        // 6. Last Resort: Split on any space
        { "CutWhere": "Before", "Pattern": "\\s" }
      ]
    },
    {
      "TranscriptionId": "final-ai-texts",
      "Enabled": true,
      // [OPTION] Set to "arbitrer-final-choice" if you want to go with multiples translation and an arbiter.
      "SourceId": "finalized_maverick"
    },
    {
      "Id": "final-ai-srt",
      "Enabled": true
    },
    {
      "Id": "final-ai-debug-srt",
      "Enabled": true
    },
    {
      "Id": "costs",
      "Enabled": true
    },
    {
      "TranscriptionId": "final-user-texts",
      // [OPTION] Set to true if you want to go import ".final-user.srt" file (final version after manual edit).
      "Enabled": false
    },
    {
      "OutputId": "learning-srt",
      // [OPTION] Set to true if you want to go import ".final-user.srt" file (final version after manual edit) and output a subtitle that, maybe, could be used to help an AI learn your preference.
      "Enabled": false
    },
  ]
}
