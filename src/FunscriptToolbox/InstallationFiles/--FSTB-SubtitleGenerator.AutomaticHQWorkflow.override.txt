{
  "Workers": [
    // -----------------------------
    // Audio extraction
    {
      "Id": "audio",
      "Enabled": true
    },
    {
      "Id": "audio-clean-waveform",
      "Enabled": true,
    },
    {
      "Id": "asig",
      "Enabled": true     
    },
    // -----------------------------
    // Creating initial manual-input source 
    {
      "Id": "full-ai",
      "Enabled": true,
      "MaxChunkDuration": "00:05:00",
      "ExportMetadataSrt": false,
      "TranscriptionToIgnorePatterns" : [ 
        "あ+[ぁ]*。",
        "ん[っー]*。",
        "うーん。"
        ]
    },
    {
      "Id": "full",
      "Enabled": true,
      // [OPTION] Change to "full-ai-refined" if you enabled it above
      "SourceId": "full-ai"
    },
    {
      "Id": "generated-manual-input-srt",
      "Enabled": true,
      "AddToFirstSubtitle": "[USER-REVISION-NEEDED] <= remove this line when revision is done\n{OngoingContext:The scene take place in...}\n{OngoingSpeakers:Woman}\nOther metadatas to use in the file:\n- GrabOnScreenText\n- VisualTraining (for visual-analyst)"
      // [OPTION] => Remove [USER-REVISION-NEEDED] in previous line if you don't want the workflow to stop, but you won't be able to do OCR, set the speakers, etc.
    },
    {
      "Id": "manual-input",
      "Enabled": true
    },
    // -----------------------------
    // AutomaticHQWorkflow: Finalizing timings and voice-texts sources
    {
      "Id": "timings",
      "Enabled": true
    },
    {
      "Id": "on-screen-texts",
      "Enabled": true
    },
    {
      "Id": "speakers",
      "Enabled": false
    },
    {
      "Id": "singlevad-ai-refined",
      // This AI will receive the audio and validate that the transcription by looking at the initial transcription and the full-ai transcription. It also receive and analyze screenshot to create a better TranslationAnalysis.
      // [OPTION] Disable if you want to reduce the API cost.
      "Enabled": true,
      "ExportMetadataSrt": false,
      // Expand the audio send to the AI because we can't trust 100% the automatic timings.
      "ExpandStart": "00:00:01",
      "ExpandEnd": "00:00:01",
      "Options": {
        "MetadataNeeded": "full-VoiceText,!SkipRefined",
        "BinaryDataExtractors": [
          {
            "OutputFieldName": "Screenshot",
            // [OPTION] Set AddContextNodes to 'false' to reduce the size of requests, or increase the Gap settings to have less nodes added. 
            // Short explanation, if there is a large gap between two subtitles, the following context nodes will be added: 
            //    <previous>.EndTime + <ContextShortGap>, ...images every <ContextLongGap>..., <current>.Start Time - <ContextShortGap>.
            "Enabled": true,
            "AddContextNodes": true,
            "ContextShortGap": "00:00:05",
            "ContextLongGap": "00:00:30"
          }
        ],
        // [OPTION] Increase the batch size to reduce the number of request but it also mean that there will be less 'thinking' time allocated per node.
        "BatchSize": 16,
        "BatchSplitWindows": 2,
        "NbContextItems": 200,
        "NbItemsMinimumReceivedToContinue": 8,
        "TextAfterAnalysis": "IMPORTANT: You will only receive the 'full-VoiceText'. The timings on those transcriptions cannot be trusted 100% so the the audio clip you will receive has been padded on each side by 1 second. Try to find the original VoiceText inside the audio clip and only re-analyze that part to see if there was some mistake done in the transcription. If you can't find it, just use the original full-VoiceText.
      }
    },
    {
      "Id": "voice-texts",
      "Enabled": true,
      "SourceId": "full"
    },    
    // -----------------------------
    // Advanced step: Allowing the user to review and override AI generated metadatas
    {
      "Id": "metadatas-srt",
      "Enabled": true
    },
    {
      "Id": "metadatas-review",
      "Enabled": false
      // [OPTION] Set to true if you want to review the metadatas generated by the AI.
    },
        
    // -----------------------------
    // Use timings, manual-input, voice-texts and AI generated metadatas to AI Translation & Arbitration
    {
      "Id": "translated-texts_maverick",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "Options": {
        "BatchSize": 75,
        "BatchSplitWindows": 5,
        "NbContextItems": 400,
        "NbItemsMinimumReceivedToContinue": 50
      }
    },
    {
      "Id": "finalized_maverick",
      "Enabled": true,
      "ExportMetadataSrt": false,
      "MaxLineLength": 50,
      "MaxMergeGapSameSpeaker": "00:00:01",
      "MaxMergeGapDialogue": "00:00:01.2000000",
      "MaxMergeDuration": "00:00:06",
      "RemoveMissing": false,
      "AddReviewIfTooLong": true,
      "SplitPatterns": [
        { "CutWhere": "After", "Pattern": "([,.?!。、！:？-]+)" },
        { "CutWhere": "Before", "Pattern": "\\b(and|but|or)\\b" },
        { "CutWhere": "Before", "Pattern": "\\s" }
      ]
    },
    {
      "TranscriptionId": "final-ai-texts",
      "Enabled": true,
      // [OPTION] Set to "arbitrer-final-choice" if you want to go with multiples translation and an arbiter.
      "SourceId": "finalized_maverick"
    },
    {
      "Id": "final-ai-srt",
      "Enabled": true
    },
    {
      "Id": "final-ai-debug-srt",
      "Enabled": true
    },
    {
      "Id": "costs",
      "Enabled": true
    },
    {
      "TranscriptionId": "final-user-texts",
      // [OPTION] Set to true if you want to go import ".final-user.srt" file (final version after manual edit).
      "Enabled": false
    },
    {
      "OutputId": "learning-srt",
      // [OPTION] Set to true if you want to go import ".final-user.srt" file (final version after manual edit) and output a subtitle that, maybe, could be used to help an AI learn your preference.
      "Enabled": false
    },
  ]
}
